{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import spacy\n",
    "from typing import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torchvision import models\n",
    "import torchvision.utils\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_processer = spacy.load('es_core_news_md')\n",
    "\n",
    "def extract_medical_texts_and_predictions(dataset):\n",
    "    medical_texts = []\n",
    "    predictions = []\n",
    "\n",
    "    for data_element in dataset:\n",
    "        medical_text = data_element['data']['text']\n",
    "        medical_texts.append(medical_text)\n",
    "\n",
    "        for result_element in data_element['predictions']:\n",
    "            predictions.append(result_element['result'])\n",
    "\n",
    "    return medical_texts, predictions\n",
    "\n",
    "\n",
    "def remove_residual_chars(text):\n",
    "    \"\"\"\n",
    "    Substitute all characters that are not letters, digits, whitespaces or allowed punctuation with a whitespace.\n",
    "    \"\"\"\n",
    "    allowed_punctuation = r'\\.,;:\\\"!'\n",
    "    pattern = f'[^{allowed_punctuation}\\\\w\\\\s]'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "\n",
    "def extract_word_positions(text):\n",
    "    pattern = re.compile(r'\\w+|[^\\w\\s]')\n",
    "    matches = pattern.finditer(text)\n",
    "\n",
    "    words_with_indices = {match.group(): {'start': match.start(), 'end': match.end()} for match in matches}\n",
    "\n",
    "    return words_with_indices\n",
    "\n",
    "\n",
    "def is_word(token):\n",
    "    \"\"\"\n",
    "    Check if the word is a punctuation mark or a whitespace.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"[a-zA-Z]|\\d\")\n",
    "    \n",
    "    return pattern.search(token)\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(base_text):\n",
    "    text = remove_residual_chars(base_text)\n",
    "    \n",
    "    doc = spanish_processer(text)\n",
    "    token_sent = [[token for token in sentence if is_word(token.text)] for sentence in doc.sents]\n",
    "\n",
    "    token_sent = clear_processed_text(token_sent)\n",
    "    \n",
    "    token_sent = [[extract_features(sentence, j) for j in range(len(sentence))] for sentence in token_sent]\n",
    "\n",
    "    return token_sent\n",
    "\n",
    "\n",
    "def extract_features(sentence, i):\n",
    "    token = sentence[i]\n",
    "    word = token.text\n",
    "\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word_lower': word.lower(),\n",
    "        'is_capitalized': word[0].isupper(),\n",
    "        'is_all_caps': word.isupper(),\n",
    "        'is_digit': word.isdigit(),\n",
    "        'word_length': len(word),\n",
    "        'contains_digits': bool(re.search(r'\\d', word)),\n",
    "        'pos': token.pos_,\n",
    "        'lemma': token.lemma_,\n",
    "        'start-end': {'start': token.idx, 'end': token.idx + len(word)}\n",
    "    }\n",
    "\n",
    "    features[\"prefix_2\"] = word[:2]\n",
    "    features[\"suffix_2\"] = word[-2:]\n",
    "\n",
    "    return features\n",
    "\n",
    "def clear_processed_text(processed_text):\n",
    "    \"\"\"\n",
    "    Remove empty sentences from the processed text. \n",
    "    \"\"\"\n",
    "    return [sentence for sentence in processed_text if sentence]\n",
    "\n",
    "def extract_cues_and_scopes(document):\n",
    "    \"\"\"\n",
    "    Extract negations, uncertain cues, negation scopes and uncertain scopes from the document.\n",
    "    \"\"\"\n",
    "    negations = [result_element for result_element in document[\"predictions\"][0][\"result\"] if \"NEG\" in result_element[\"value\"][\"labels\"]]\n",
    "    uncertains = [result_element for result_element in document[\"predictions\"][0][\"result\"] if \"UNC\" in result_element[\"value\"][\"labels\"]]\n",
    "    nscopes = [result_element for result_element in document[\"predictions\"][0][\"result\"] if \"NSCO\" in result_element[\"value\"][\"labels\"]]\n",
    "    uscopes = [result_element for result_element in document[\"predictions\"][0][\"result\"] if \"USCO\" in result_element[\"value\"][\"labels\"]]\n",
    "\n",
    "    # Sort the cues and scopes by their start position\n",
    "    negations.sort(key=lambda x: x[\"value\"][\"start\"])\n",
    "    uncertains.sort(key=lambda x: x[\"value\"][\"start\"])\n",
    "    nscopes.sort(key=lambda x: x[\"value\"][\"start\"])\n",
    "    uscopes.sort(key=lambda x: x[\"value\"][\"start\"])\n",
    "\n",
    "    return negations, uncertains, nscopes, uscopes\n",
    "\n",
    "\n",
    "def BIO_tagging(tokens, labels, label_name, original_text):\n",
    "    \"\"\"\n",
    "    Set BIO tags for the tokens based on the labels from the training data.\n",
    "    \n",
    "    Args:\n",
    "    - tokens (list): A list of token dictionaries\n",
    "    - labels (list): A list of labels from the training data\n",
    "    - label_name (str): The name of the label to use for tagging (e.g., \"NEG\")\n",
    "    - original_text (str): The original text of the document\n",
    "    \"\"\"\n",
    "     \n",
    "    negation_idx = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if 'tag' not in token:\n",
    "            token['tag'] = 'O'\n",
    "\n",
    "        if negation_idx >= len(labels):\n",
    "            continue\n",
    "\n",
    "        negation = labels[negation_idx]\n",
    "\n",
    "        neg_start = negation['value']['start']\n",
    "        neg_end = negation['value']['end']\n",
    "\n",
    "        # Skip whitespace characters at the start and end positions\n",
    "        if original_text[neg_start] == ' ':\n",
    "            neg_start += 1\n",
    "        if original_text[neg_end - 1] == ' ':\n",
    "            neg_end -= 1\n",
    "\n",
    "        token_start = token['start-end']['start']\n",
    "        token_end = token['start-end']['end']\n",
    "\n",
    "        if token_start == neg_start:\n",
    "            token['tag'] = f'B-{label_name}'\n",
    "        elif neg_start < token_start < neg_end:\n",
    "            # If the token is at the end of the negation, set the tag to E\n",
    "            if token_end == neg_end:\n",
    "                token['tag'] = f'E-{label_name}'\n",
    "            else:\n",
    "                token['tag'] = f'I-{label_name}'\n",
    "        elif token_start > neg_end:\n",
    "            # Move to the next negation that starts after the current token\n",
    "            while negation_idx < len(labels) - 1 and token_start > labels[negation_idx]['value']['start']:\n",
    "                negation_idx += 1\n",
    "                negation = labels[negation_idx]\n",
    "            \n",
    "            if token_start == negation['value']['start']:\n",
    "                token['tag'] = f'B-{label_name}'\n",
    "    \n",
    "def add_BIO_tags(X_train_texts, Y_train, medical_texts_train, verbose=False):\n",
    "    \"\"\"\n",
    "    Set BIO tags for the tokens in the training data based on the labels from the training data.\n",
    "    \n",
    "    Args:\n",
    "    - X_train_texts (list): A list of tokenized sentences\n",
    "    - Y_train (list): A list of labels from the training data\n",
    "    - medical_texts_train (list): A list of medical texts\n",
    "    - verbose (bool): Whether to print the tagged sentences\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    \n",
    "    for text_idx, (train_text, train_labels, original_text) in enumerate(list(zip(X_train_texts, Y_train, medical_texts_train))):\n",
    "        negs, uncs, nscs, uscs = train_labels\n",
    "        for i in range(len(train_text)):\n",
    "            BIO_tagging(train_text[i], negs, \"NEG\", original_text)\n",
    "            BIO_tagging(train_text[i], uncs, \"UNC\", original_text)\n",
    "            BIO_tagging(train_text[i], nscs, \"NSCO\", original_text)\n",
    "            BIO_tagging(train_text[i], uscs, \"USCO\", original_text)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Text: \", text_idx, \" : \", original_text[:100], \"...\")\n",
    "            for i, sentence in enumerate(train_text[:2]):\n",
    "                print(\"Sentence: \", i)\n",
    "                for token in sentence[:5]:\n",
    "                    print(token)\n",
    "            print(\"<--------------------------------------->\")    \n",
    "            \n",
    "    return X_train_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the data from files, tokenizing and tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text and extracting cues...\n",
      "Tagging tokens...\n",
      "Number of training documents: 254\n",
      "Number of test documents: 64\n",
      "Limiting to -1 documents...\n"
     ]
    }
   ],
   "source": [
    "with open('train_data.json', 'r', encoding='utf-8') as train_file:\n",
    "    train_dataset = json.load(train_file)\n",
    "\n",
    "    with open('test_data.json', 'r', encoding='utf-8') as test_file:\n",
    "        test_dataset = json.load(test_file)\n",
    "\n",
    "    medical_texts_train, predictions_train = extract_medical_texts_and_predictions(train_dataset)\n",
    "    medical_texts_test, predictions_test = extract_medical_texts_and_predictions(test_dataset)\n",
    "\n",
    "    limit = -1  # Limit the number of documents to process (set to -1 to process all documents)\n",
    "\n",
    "    print(\"Preprocessing text and extracting cues...\")\n",
    "\n",
    "    processed_text_train = [preprocess_and_tokenize(text) for text in medical_texts_train[:limit]]\n",
    "    processed_text_test = [preprocess_and_tokenize(text) for text in medical_texts_test[:limit]]\n",
    "    \n",
    "    Y_train = [extract_cues_and_scopes(document) for document in train_dataset[:limit]]\n",
    "    Y_test = [extract_cues_and_scopes(document) for document in test_dataset[:limit]]\n",
    "\n",
    "    print(\"Tagging tokens...\")\n",
    "\n",
    "    tagged_texts_train = add_BIO_tags(processed_text_train, Y_train, medical_texts_train)\n",
    "    tagged_texts_test = add_BIO_tags(processed_text_test, Y_test, medical_texts_test)\n",
    "   \n",
    "    print(\"Number of training documents:\", len(medical_texts_train))\n",
    "    print(\"Number of test documents:\", len(medical_texts_test))\n",
    "    print(\"Limiting to\", limit, \"documents...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([token['word'] for token in tagged_texts_train[0][0]])\n",
    "print([token['word'] for token in tagged_texts_test[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train sentences\n",
    "tmp = []\n",
    "\n",
    "for text in tagged_texts_train:\n",
    "    tmp.extend(text)\n",
    "    \n",
    "tagged_sentences_train = tmp\n",
    "\n",
    "# Concatenate test sentences\n",
    "\n",
    "tmp = []\n",
    "\n",
    "for text in tagged_texts_test:\n",
    "    tmp.extend(text)\n",
    "    \n",
    "tagged_sentences_test = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        #setting the pre-reserved tokens int to string tokens\n",
    "        self.ind2word = {0:\"<pad>\",1:\"<start>\",2:\"<end>\",3:\"<unk>\",4:\"<date>\"}\n",
    "        \n",
    "        #string to int tokens\n",
    "        #its reverse dict self.itos\n",
    "        self.word2ind = {v:k for k,v in self.ind2word.items()}\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        tokens = []\n",
    "        \n",
    "        for token in text:\n",
    "            # If it is a date, replace it with <date> token\n",
    "            if token['pos'] == 'NUM' and re.match(r'\\d{2,4}(-|\\/|.)\\d{2}(-|\\/|.)\\d{2,4}', token['word']):\n",
    "                tokens.append('<date>')\n",
    "            else:\n",
    "                tokens.append(token['word_lower'])\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 5\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                #add the word to the vocab if it reaches minimum frequecy threshold\n",
    "                if frequencies[word] == self.freq_threshold and word not in self.word2ind:\n",
    "                    self.word2ind[word] = idx\n",
    "                    self.ind2word[idx] = word\n",
    "                    idx += 1\n",
    "                \n",
    "    def numericalize(self,text):\n",
    "        # change word with index\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [ self.word2ind[token] if token in self.word2ind else self.word2ind[\"<unk>\"] for token in tokenized_text ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = Vocabulary(freq_threshold=1)\n",
    "word_vocab.build_vocab(tagged_sentences_train)\n",
    "print(\"Word vocab size: \", len(word_vocab.word2ind))\n",
    "print(word_vocab.word2ind)\n",
    "print([token['word_lower'] for token in tagged_sentences_train[0]])\n",
    "print(word_vocab.numericalize(tagged_sentences_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tag vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        #setting the pre-reserved tokens int to string tokens\n",
    "        self.ind2word = {0:\"<pad>\",1:\"<start>\",2:\"<end>\",3:\"<unk>\"}\n",
    "        \n",
    "        #string to int tokens\n",
    "        #its reverse dict self.itos\n",
    "        self.word2ind = {v:k for k,v in self.ind2word.items()}\n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        tokens = []\n",
    "        \n",
    "        for token in text:\n",
    "            tokens.append(token['pos'])\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                #add the word to the vocab if it reaches minimum frequecy threshold\n",
    "                if frequencies[word] == self.freq_threshold and word not in self.word2ind:\n",
    "                    self.word2ind[word] = idx\n",
    "                    self.ind2word[idx] = word\n",
    "                    idx += 1\n",
    "                \n",
    "    def numericalize(self,text):\n",
    "        # change word with index\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [ self.word2ind[token] if token in self.word2ind else self.word2ind[\"<unk>\"] for token in tokenized_text ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vocab = POS_Vocabulary(freq_threshold=1)\n",
    "pos_vocab.build_vocab(tagged_sentences_train)\n",
    "print(\"POS vocab size: \", len(pos_vocab.word2ind))\n",
    "print(pos_vocab.word2ind)\n",
    "print([token['pos'] for token in tagged_sentences_train[0]])\n",
    "print(pos_vocab.numericalize(tagged_sentences_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character vocabulary(set of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all characters in the vocabulary\n",
    "chars = set('\\0')   # Start with the null character which is used for padding\n",
    "longest_word = 0\n",
    "\n",
    "for sentence in tagged_sentences_train:\n",
    "    for token in sentence:\n",
    "        # Check if the current word is the longest one\n",
    "        if len(token['word_lower']) > longest_word:\n",
    "            longest_word = len(token['word_lower'])\n",
    "            \n",
    "        # Add all characters in the word to the set\n",
    "        chars.update(token['word_lower'])\n",
    "        \n",
    "        \n",
    "chars = list(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2ind(sentence):\n",
    "    \"\"\"\n",
    "    Convert the characters in the sentence to their corresponding indices in the character vocabulary.\n",
    "    \"\"\"\n",
    "    sentence_chars = []\n",
    "    \n",
    "    for token in sentence:\n",
    "        token_chars = []\n",
    "        \n",
    "        for char in token['word_lower']:\n",
    "            token_chars.append(chars.index(char))\n",
    "        \n",
    "        # Add padding to the end of the word\n",
    "        for _ in range(longest_word - len(token['word_lower'])):\n",
    "            token_chars.append(0)\n",
    "        \n",
    "        sentence_chars.append(token_chars)\n",
    "        \n",
    "    return sentence_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2casings(sentence):\n",
    "    \"\"\"\n",
    "    Create one hot encoding for the casing of the token.\n",
    "\n",
    "    The casing features are:\n",
    "    - Contains digits\n",
    "    - Contains punctuation\n",
    "    - First word in the sentence\n",
    "    - Last word in the sentence\n",
    "    \"\"\"\n",
    "    casings = []\n",
    "    \n",
    "    for idx, token in enumerate(sentence):\n",
    "        casing = []\n",
    "        casing.append(1 if any(char.isdigit() for char in token['word_lower']) else 0)\n",
    "        casing.append(1 if any(not char.isalnum() for char in token['word_lower']) else 0)\n",
    "        casing.append(1 if idx == 0 else 0)\n",
    "        casing.append(1 if idx == len(sentence) - 1 else 0)\n",
    "        casings.append(casing)\n",
    "        \n",
    "    return casings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence: 268\n"
     ]
    }
   ],
   "source": [
    "# Longest sentence in the processed text with start and end tokens\n",
    "longest_sentence = max(len(sentence) for sentence in tagged_sentences_train) + 2\n",
    "print(\"Longest sentence:\", longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second longest sentence: 259\n"
     ]
    }
   ],
   "source": [
    "# Find second longest sentence\n",
    "sorted_sentences = sorted(tagged_sentences_train, key=lambda x: len(x), reverse=True)\n",
    "second_longest_sentence = len(sorted_sentences[1]) + 2\n",
    "print(\"Second longest sentence:\", second_longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of long sentences: 1\n"
     ]
    }
   ],
   "source": [
    "# Count the number of long sentences\n",
    "long_sentences = [sentence for sentence in tagged_sentences_train if len(sentence) + 2 == longest_sentence]\n",
    "print(\"Number of long sentences:\", len(long_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find longest sentence with its idx\n",
    "longest_sentence_idx = np.argmax([len(sentence) for sentence in tagged_sentences_train])\n",
    "print(\"Longest sentence idx:\", longest_sentence_idx)\n",
    "print([token['word'] for token in tagged_sentences_train[longest_sentence_idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIO Tag vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag_Vocabulary:\n",
    "    def __init__(self):\n",
    "        #setting the pre-reserved tokens int to string tokens\n",
    "        self.ind2word = {0:\"<pad>\",1:\"<start>\",2:\"<end>\",3:\"<unk>\",4:\"O\",5:\"B-NEG\",6:\"I-NEG\",7:\"E-NEG\",8:\"B-UNC\",9:\"I-UNC\",10:\"E-UNC\",11:\"B-NSCO\",12:\"I-NSCO\",13:\"E-NSCO\",14:\"B-USCO\",15:\"I-USCO\",16:\"E-USCO\"}\n",
    "        \n",
    "        #string to int tokens\n",
    "        #its reverse dict self.itos\n",
    "        self.word2ind = {v:k for k,v in self.ind2word.items()}\n",
    "        \n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        tokens = []\n",
    "        \n",
    "        for token in text:\n",
    "            tokens.append(token['tag'])\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def numericalize(self,text):\n",
    "        # change word with index\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [ self.word2ind[token] if token in self.word2ind else self.word2ind[\"O\"] for token in tokenized_text ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_vocab = Tag_Vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_sents(sentences):\n",
    "    \"\"\"\n",
    "    Convert the sentences to numerical form.\n",
    "    \"\"\"\n",
    "    numericalized_sents = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        numericalized_sent = []\n",
    "        \n",
    "        word_encoding = word_vocab.numericalize(sentence)\n",
    "        pos_encoding = pos_vocab.numericalize(sentence)\n",
    "        char_encoding = char2ind(sentence)\n",
    "        casing_encoding = word2casings(sentence)\n",
    "        \n",
    "        numericalized_sent = list(zip(word_encoding, pos_encoding, char_encoding, casing_encoding))\n",
    "        \n",
    "        # Add start and end tokens\n",
    "        numericalized_sent.insert(0, [word_vocab.word2ind['<start>'], pos_vocab.word2ind['<start>'], [0] * longest_word, [0, 0, 0, 0]])\n",
    "        numericalized_sent.append([word_vocab.word2ind['<end>'], pos_vocab.word2ind['<end>'], [0] * longest_word, [0, 0, 0, 0]])\n",
    "        \n",
    "        # Fill with padding\n",
    "        for _ in range(longest_sentence - len(numericalized_sent)):\n",
    "            numericalized_sent.append([ word_vocab.word2ind['<pad>'], pos_vocab.word2ind['<pad>'], [0] * longest_word, [0, 0, 0, 0]])\n",
    "            \n",
    "        numericalized_sents.append(numericalized_sent)\n",
    "        \n",
    "    return numericalized_sents\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the tagged sentences to numericalized format\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "X_train = numericalize_sents(tagged_sentences_train)\n",
    "X_test = numericalize_sents(tagged_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(X_test[0])\n",
    "\n",
    "word_sent_train = [word_vocab.ind2word[word] for word, _, _, _ in X_train[0]]\n",
    "word_sent_test = [word_vocab.ind2word[word] for word, _, _, _ in X_test[0]]\n",
    "\n",
    "original_sent_train = [token['word'] for token in tagged_sentences_train[0]]\n",
    "original_sent_test = [token['word'] for token in tagged_sentences_test[0]]\n",
    "\n",
    "print(word_sent_train)\n",
    "print(word_sent_test)\n",
    "\n",
    "print(original_sent_train)\n",
    "print(original_sent_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the tags to numericalized format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_tags(tagged_sentences, tag_vocab):\n",
    "    Y = []\n",
    "\n",
    "    # Transform the tags to numericalized format\n",
    "    for sentence in tagged_sentences:\n",
    "        sentence_tags = []\n",
    "        \n",
    "        tag_encoding = tag_vocab.numericalize(sentence)\n",
    "        \n",
    "        for i, token in enumerate(sentence):\n",
    "            sentence_tags.append(tag_encoding[i])\n",
    "            \n",
    "        # Add start and end tags to the sentence\n",
    "        start_token = tag_vocab.word2ind['<start>']\n",
    "        end_token = tag_vocab.word2ind['<end>']\n",
    "\n",
    "        sentence_tags.insert(0, start_token)\n",
    "        sentence_tags.append(end_token)\n",
    "        \n",
    "        # Fill with padding to the longest sentence\n",
    "        for _ in range(longest_sentence - len(sentence_tags)):\n",
    "            padding_token = tag_vocab.word2ind['<pad>']\n",
    "            sentence_tags.append(padding_token)\n",
    "            \n",
    "        Y.append(sentence_tags)\n",
    "        \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "Y_train = numericalize_tags(tagged_sentences_train, tag_vocab)\n",
    "Y_test = numericalize_tags(tagged_sentences_test, tag_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0][:3])\n",
    "print(Y_train[2])\n",
    "\n",
    "word_sent_test = [word_vocab.ind2word[word] for word, _, _, _ in X_test[0]]\n",
    "\n",
    "print(word_sent_test[45:55])\n",
    "print(Y_test[0][45:55])\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create the dataloaders\n",
    "\n",
    "# Custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.Y[idx]\n",
    "        \n",
    "        # Word encoding tensor\n",
    "        word_tensor = torch.tensor([x[0] for x in X[:longest_sentence]], dtype=torch.long)\n",
    "        \n",
    "        # POS encoding tensor\n",
    "        pos_tensor = torch.tensor([x[1] for x in X[:longest_sentence]], dtype=torch.long)\n",
    "        \n",
    "        # Character encoding tensor.\n",
    "        char_tensor = torch.tensor([x[2][:longest_word] for x in X[:longest_sentence]], dtype=torch.long)\n",
    "        \n",
    "        # Casing encoding tensor\n",
    "        casing_tensor = torch.tensor([x[3] for x in X[:longest_sentence]], dtype=torch.long)\n",
    "                \n",
    "        # Convert y to tensor\n",
    "        y_tensor = torch.tensor(y[:longest_sentence], dtype=torch.long)\n",
    "        \n",
    "        return word_tensor, pos_tensor, char_tensor, casing_tensor, y_tensor\n",
    "    \n",
    "train_dataset = CustomDataset(X_train, Y_train)\n",
    "test_dataset = CustomDataset(X_test, Y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[4])\n",
    "print(Y_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest value of X_pos\n",
    "X_pos = [x[1] for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tensor shape: torch.Size([64, 268])\n",
      "POS tensor shape: torch.Size([64, 268])\n",
      "Character tensor shape: torch.Size([64, 268, 28])\n",
      "Casing tensor shape: torch.Size([64, 268, 4])\n",
      "Tag tensor shape: torch.Size([64, 268])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of data\n",
    "X_word, X_pos, X_char, X_casing, y_batch = next(iter(train_dataloader))\n",
    "\n",
    "print(\"Word tensor shape:\", X_word.shape)\n",
    "print(\"POS tensor shape:\", X_pos.shape)\n",
    "print(\"Character tensor shape:\", X_char.shape)\n",
    "print(\"Casing tensor shape:\", X_casing.shape)\n",
    "print(\"Tag tensor shape:\", y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[0])\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,vocab_dim_ch,vocab_dim_pos,vocab_dim_word, max_sent_legnth, max_Xch_len,len_X_casing, embedding_dim_ch_pos,embedding_dim, hidden_dim, n_layers,tag_dim, drop_prob=0.):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, tag_dim)\n",
    "\n",
    "        self.prelu = nn.PReLU()\n",
    "        #####################################33\n",
    "        self.embedding_ch = nn.Embedding(vocab_dim_ch,embedding_dim_ch_pos)\n",
    "        self.conv2d = nn.Conv2d(in_channels=max_sent_legnth,out_channels=max_sent_legnth,kernel_size=3)\n",
    "        self.dropout2d = nn.Dropout2d(p=0.5)\n",
    "        self.maxpool = nn.MaxPool2d(2,stride = 2)\n",
    "        self.flatten = nn.Flatten(2,3)\n",
    "        self.fc_ch = nn.Linear(312,300)\n",
    "        self.fc_pos = nn.Linear(embedding_dim_ch_pos,300)\n",
    "        self.fc_one_hot = nn.Linear(len_X_casing,300)\n",
    "\n",
    "        print(\"Voacb dim pos\", vocab_dim_pos)\n",
    "        print(\"Voacb dim word\", vocab_dim_word)\n",
    "        print(\"embedding_dim_ch_pos\", embedding_dim_ch_pos)\n",
    "\n",
    "        self.embedding_pos = nn.Embedding(vocab_dim_pos,embedding_dim_ch_pos)\n",
    "        self.embedding_word =nn.Embedding(vocab_dim_word, embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, h, c=None):\n",
    "        emb_0 = self.embedding_ch(x[0])\n",
    "        \n",
    "        out_0 = self.conv2d(emb_0)\n",
    "        \n",
    "        out_0 = self.prelu(out_0)\n",
    "        out_0 = self.dropout2d(out_0)\n",
    "        out_0 = self.maxpool(out_0)\n",
    "        \n",
    "        out_0 = self.flatten(out_0)\n",
    "        \n",
    "        out_0 = self.fc_ch(out_0)\n",
    "        out_0 = self.prelu(out_0)\n",
    "\n",
    "        torch.set_printoptions(threshold=10_000)\n",
    "\n",
    "        out_1 = self.embedding_pos(x[1])\n",
    "        # out_1 = self.fc_pos(out_1)\n",
    "        # out_1 = self.prelu(out_1)\n",
    "\n",
    "        out_2 = self.embedding_word(x[2])\n",
    "\n",
    "        out_3 = self.fc_one_hot(x[3])\n",
    "        out_3 = self.prelu(out_3)\n",
    "\n",
    "        # print(\"out_0 shape\", out_0.shape)\n",
    "        # print(\"out_1 shape\", out_1.shape)\n",
    "        # print(\"out_2 shape\", out_2.shape)\n",
    "        # print(\"out_3 shape\", out_3.shape)\n",
    "\n",
    "        # out = out_0 + out_1 + out_2 + out_3\n",
    "        out = out_0  + out_2 + out_3\n",
    "\n",
    "        # print(\"h shape\", h.shape)\n",
    "        # print(\"c shape\", c.shape)   \n",
    "        \n",
    "        out, (h, c) = self.lstm(out, (h, c))\n",
    "        \n",
    "        # print(\"out shape\", out.shape)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.prelu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out, h, c\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \" Initialize the hidden state of the LSTM to zeros\"\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers * 2, batch_size, self.hidden_dim).zero_(), weight.new(self.n_layers * 2, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, batch_size, num_epochs, learning_rate):\n",
    "    model.train()\n",
    "\n",
    "    clas_weights = torch.tensor([\n",
    "        0.1,  # <pad>\n",
    "        1.0,  # <start>\n",
    "        1.0,  # <end>\n",
    "        1.0,  # <unk>\n",
    "        0.5,  # O\n",
    "        1.0,  # B-NEG\n",
    "        1.0,  # I-NEG\n",
    "        1.0,  # E-NEG\n",
    "        1.0,  # B-UNC\n",
    "        1.0,  # I-UNC\n",
    "        1.0,  # E-UNC\n",
    "        1.0,  # B-NSCO\n",
    "        1.0,  # I-NSCO\n",
    "        1.0,  # E-NSCO\n",
    "        1.0,  # B-USCO\n",
    "        1.0,  # I-USCO\n",
    "        1.0   # E-USCO\n",
    "    ])\n",
    "    \n",
    "    clas_weights = clas_weights.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=clas_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for batch, (X_word, X_pos, X_char, X_casing, y) in enumerate(dataloader):\n",
    "\n",
    "            h_state, c_state = model.init_hidden(X_word.shape[0])\n",
    "            h_state = h_state.to(device)\n",
    "\n",
    "            if c_state is not None:\n",
    "                c_state = c_state.to(device)\n",
    "\n",
    "            X_casing = X_casing.float()\n",
    "\n",
    "            X_word = X_word.to(device)\n",
    "            X_pos = X_pos.to(device)\n",
    "            X_char = X_char.to(device)\n",
    "            X_casing = X_casing.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Find largest value from X_pos\n",
    "            max_pos = torch.max(X_pos)\n",
    "\n",
    "            y_pred, h_state, c_state = model((X_char, X_pos, X_word, X_casing),  h_state, c_state) # in LSTM we have a cell state and a hidden state\n",
    "\n",
    "            y_pred = y_pred.view(-1, y_pred.shape[2])\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch%30 == 0:\n",
    "                print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "                losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dim_ch=len(chars)\n",
    "vocab_dim_pos=len(pos_vocab.word2ind)\n",
    "vocab_dim_word=len(word_vocab.word2ind)\n",
    "max_Xch_len=longest_word\n",
    "len_X_casing=4  # casing features\n",
    "embedding_dim_ch_pos = 50\n",
    "embedding_dim = 300\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "tag_dim = len(tag_vocab.word2ind)\n",
    "drop_prob= 0.4\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = Model(vocab_dim_ch,vocab_dim_pos,vocab_dim_word, longest_sentence, max_Xch_len,len_X_casing, embedding_dim_ch_pos,embedding_dim, hidden_dim, n_layers,tag_dim, drop_prob).to(device)\n",
    "losses = train(train_dataloader, model, batch_size, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f71f36dfa30>]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the losses\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log scale losses\n",
    "ax = plt.figure()\n",
    "ax.set_yscale('log')\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    X_word, X_pos, X_char, X_casing, y_batch = next(iter(test_dataloader))\n",
    "    \n",
    "    # Convert to float\n",
    "    # X_char = X_char.float()\n",
    "    X_casing = X_casing.float()\n",
    "    \n",
    "    h_state, c_state = model.init_hidden(batch_size)\n",
    "    \n",
    "    print(\"Casing type:\", X_casing.dtype)\n",
    "    \n",
    "    X_char = X_char.to(device)\n",
    "    X_pos = X_pos.to(device)\n",
    "    X_word = X_word.to(device)\n",
    "    X_casing = X_casing.to(device)\n",
    "    h_state = h_state.to(device)\n",
    "    c_state = c_state.to(device)\n",
    "    \n",
    "    pred, h_state, c_state = model((X_char, X_pos, X_word, X_casing), h_state, c_state)\n",
    "    \n",
    "    pred = torch.argmax(pred, dim=2)\n",
    "    \n",
    "    print(\"Predictions shape:\", pred.shape)\n",
    "    print(\"Ground truth shape:\", y_batch.shape)\n",
    "    \n",
    "    idx = 37\n",
    "    \n",
    "    print(\"Predictions:\", pred[idx])\n",
    "    print(\"Ground truth:\", y_batch[idx])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get item 24 from the loader\n",
    "X_word, X_pos, X_char, X_casing, y_batch = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9710789063487383\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    for X_word, X_pos, X_char, X_casing, y_batch in test_dataloader:\n",
    "        X_casing = X_casing.float()\n",
    "        \n",
    "        h_state, c_state = model.init_hidden(X_word.shape[0])\n",
    "        \n",
    "        X_char = X_char.to(device)\n",
    "        X_pos = X_pos.to(device)\n",
    "        X_word = X_word.to(device)\n",
    "        X_casing = X_casing.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        h_state = h_state.to(device)\n",
    "        c_state = c_state.to(device)\n",
    "        \n",
    "        pred, h_state, c_state = model((X_char, X_pos, X_word, X_casing), h_state, c_state)\n",
    "        \n",
    "        pred = torch.argmax(pred, dim=2)\n",
    "                \n",
    "        # Count the number of correct predictions for labeled tokens. Ignore padding tokens.\n",
    "        for i in range(len(y_batch)):\n",
    "            for j in range(len(y_batch[i])):\n",
    "                if y_batch[i][j] > 2:\n",
    "                    total += 1\n",
    "                    if y_batch[i][j] == pred[i][j]:\n",
    "                        correct += 1\n",
    "                        labeled_correct += 1\n",
    "                        \n",
    "        \n",
    "print(\"Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall and F1 score with scikit-learn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "predicts = []\n",
    "ground_truths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    for X_word, X_pos, X_char, X_casing, y_batch in train_dataloader:\n",
    "        X_casing = X_casing.float()\n",
    "        \n",
    "        h_state, c_state = model.init_hidden(X_word.shape[0])\n",
    "        \n",
    "        X_char = X_char.to(device)\n",
    "        X_pos = X_pos.to(device)\n",
    "        X_word = X_word.to(device)\n",
    "        X_casing = X_casing.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        h_state = h_state.to(device)\n",
    "        c_state = c_state.to(device)\n",
    "        \n",
    "        pred, h_state, c_state = model((X_char, X_pos, X_word, X_casing), h_state, c_state)\n",
    "        \n",
    "        pred = torch.argmax(pred, dim=2)\n",
    "        \n",
    "        # Flatten the tensors\n",
    "        y_batch = y_batch.view(-1)\n",
    "        pred = pred.view(-1)\n",
    "        \n",
    "        # Ignore padding, start and end tokens\n",
    "        # mask = (y_batch > 2)\n",
    "        \n",
    "        # y_batch = y_batch[mask]\n",
    "        # pred = pred[mask]\n",
    "        \n",
    "        predicts.extend(pred.cpu().numpy())\n",
    "        ground_truths.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "print(\"F1 score:\", f1_score(ground_truths, predicts, average='weighted'))\n",
    "print(\"Precision:\", precision_score(ground_truths, predicts, average='weighted'))\n",
    "print(\"Recall:\", recall_score(ground_truths, predicts, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <pad>       1.00      1.00      1.00   3022358\n",
      "     <start>       1.00      1.00      1.00     12070\n",
      "       <end>       1.00      1.00      1.00     12070\n",
      "       <unk>       0.00      0.00      0.00         0\n",
      "           O       1.00      1.00      1.00    168832\n",
      "       B-NEG       0.99      1.00      1.00      4281\n",
      "       I-NEG       0.00      0.00      0.00         0\n",
      "       E-NEG       1.00      0.93      0.97        91\n",
      "       B-UNC       1.00      0.99      0.99       455\n",
      "       I-UNC       0.00      0.00      0.00         5\n",
      "       E-UNC       0.99      0.98      0.99       214\n",
      "      B-NSCO       0.99      1.00      0.99      4063\n",
      "      I-NSCO       0.98      0.96      0.97      6579\n",
      "      E-NSCO       0.87      0.95      0.91      1965\n",
      "      B-USCO       0.98      0.98      0.98       449\n",
      "      I-USCO       0.98      0.99      0.98      1216\n",
      "      E-USCO       0.93      0.77      0.84       112\n",
      "\n",
      "   micro avg       1.00      1.00      1.00   3234760\n",
      "   macro avg       0.81      0.80      0.80   3234760\n",
      "weighted avg       1.00      1.00      1.00   3234760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(tag_vocab.ind2word.values())\n",
    "\n",
    "print(classification_report(ground_truths, predicts, target_names=labels, zero_division=0, labels=range(len(labels))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
